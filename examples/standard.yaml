# A standard training run, should almost never break
standard: !Experiment
  xnmt_global: !XnmtGlobal
    model_file: examples/output/{EXP}.mod
    out_file: examples/output/{EXP}.out
    err_file: examples/output/{EXP}.err
    default_layer_dim: 512
    dropout: 0.3
  model: !DefaultTranslator
    src_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: examples/data/head.ja.vocab}
    trg_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: examples/data/head.en.vocab}
    src_embedder: !SimpleWordEmbedder {}
    encoder: !BiLSTMSeqTransducer
      layers: 1
    attender: !MlpAttender {}
    trg_embedder: !SimpleWordEmbedder {}
    decoder: !MlpSoftmaxDecoder
      layers: 1
      bridge: !CopyBridge {}
    inference: !SimpleInference
      len_norm_type: !PolynomialNormalization
        apply_during_search: true
        m: 5
  train: !SimpleTrainingRegimen
    trainer: !AdamTrainer
      alpha: 0.001
    run_for_epochs: 20
    src_file: examples/data/head.ja
    trg_file: examples/data/head.en
    dev_tasks:
      - !LossEvalTask
        src_file: examples/data/head.ja
        ref_file: examples/data/head.en
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu,wer
      src_file: examples/data/head.ja
      ref_file: examples/data/head.en
      hyp_file: examples/output/{EXP}.test_hyp
