defaults:
    experiment:
        model_file: examples/output/<EXP>.mod
        hyp_file: examples/output/<EXP>.hyp
        out_file: examples/output/<EXP>.out
        err_file: examples/output/<EXP>.err
        run_for_epochs: 20
        eval_metrics: recall|nbest=5
    train:
        default_layer_dim: 512
        dropout: 0.0
        dev_metrics: recall|nbest=5
        training_corpus: !BilingualTrainingCorpus
            train_src: examples/data/mfcc_tr20.npz
            train_trg: examples/data/flickr_tr20.ids
            dev_src: examples/data/mfcc_tx20.npz
            dev_trg: examples/data/flickr_tx20.ids
        corpus_parser: !BilingualCorpusParser
            src_reader: !ContVecReader {}
            trg_reader: !IDReader {}
        model: !DotProductRetriever
            src_embedder: !NoopEmbedder
                #vocab_size: 5000 # TODO: set this automatically
                emb_dim: 40
            src_encoder: !HarwathSpeechEncoder
                filter_height: [40, 1, 1]
                filter_width: [5, 25, 25]
                channels: [1, 64, 512]
                num_filters: [64, 512, 1024]
                stride: [1, 1, 1]
            trg_embedder: !NoopEmbedder
                emb_dim: 40
            trg_encoder: !HarwathImageEncoder
                in_height: 4096
                out_height: 1024 
            database: !StandardRetrievalDatabase
                reader: !ContVecReader {}
                database_file: examples/data/vgg20.npz
    decode:
        src_file: examples/data/mfcc_tx20.npz
    evaluate:
        ref_file: examples/data/flickr_tx20.ids

standard-dropout0.1:
    train:
       dropout: 0.1
