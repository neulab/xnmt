defaults:
  experiment:
    model_file: examples/output/<EXP>.mod
    hyp_file: examples/output/<EXP>.hyp
    out_file: examples/output/<EXP>.out
    err_file: examples/output/<EXP>.err
    run_for_epochs: 2
    eval_metrics: recall|nbest=1,recall|nbest=5
  train:
    trainer: 'adam'
    learning_rate: 0.01 
    default_layer_dim: 512
    dropout: 0.0
    dev_metrics: recall|nbest=1,recall|nbest=5
    training_corpus: !BilingualTrainingCorpus
      train_src: examples/data/mfcc_tr20.npz
      train_trg: examples/data/flickr_tr20.ids
      dev_src: examples/data/mfcc_tx20.npz
      dev_trg: examples/data/flickr_tx20.ids
    corpus_parser: !BilingualCorpusParser
      src_reader: !ContVecReader {}
      trg_reader: !IDReader {}
    model: !DotProductRetriever
      src_embedder: !NoopEmbedder
        emb_dim: 40
      src_encoder: !TilburgSpeechSeqTransducer
        filter_height: 40
        filter_width: 6
        channels: 1
        num_filters: 64
        stride: 2
        rhn_num_hidden_layers: 2
        rhn_dim: 1024
        rhn_microsteps: 2
        attention_dim: 128
        residual: True
      trg_embedder: !NoopEmbedder
        emb_dim: 40
      trg_encoder: !HarwathImageEncoder
        in_height: 4096
        out_height: 1024 
      database: !StandardRetrievalDatabase
        reader: !ContVecReader {}
        database_file: examples/data/vgg20.npz
  decode:
    src_file: examples/data/mfcc_tx20.npz
  evaluate:
    ref_file: examples/data/flickr_tx20.ids

standard-dropout0.1:
  train:
    dropout: 0.1
