# This example demonstrates how to use the Transformer architecture following
# https://arxiv.org/abs/1706.03762
transformer: !Experiment
  exp_global: !ExpGlobal
    dropout: 0.2
    default_layer_dim: 512
  model: !TransformerTranslator
    src_embedder: !SimpleWordEmbedder
      param_init: !LeCunUniformInitializer {}
    encoder: !TransformerEncoder
      layers: 1
    trg_embedder: !SimpleWordEmbedder
      param_init: !LeCunUniformInitializer {}
    decoder: !TransformerDecoder
      layers: 1
    src_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: examples/data/head.ja.vocab}
    trg_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: examples/data/head.en.vocab}
    inference: !AutoRegressiveInference {}
  train: !SimpleTrainingRegimen
    run_for_epochs: 30
    batcher: !SentShuffleBatcher
      batch_size: 100
    restart_trainer: False
    trainer: !NoamTrainer
      alpha: 1.0
      warmup_steps: 4000
    lr_decay: 1.0
    src_file: examples/data/train-big.ja
    trg_file: examples/data/train-big.en
    dev_tasks:
      - !AccuracyEvalTask
        eval_metrics: bleu
        src_file: examples/data/dev.ja
        ref_file: examples/data/test.en
        hyp_file: examples/output/{EXP}.test_hyp
      - !LossEvalTask
        src_file: examples/data/dev.ja
        ref_file: examples/data/dev.en
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: examples/data/dev.ja
      ref_file: examples/data/test.en
      hyp_file: examples/output/{EXP}.test_hyp
    
