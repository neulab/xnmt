# A setup using self-attention
self_attention: !Experiment
  exp_global: !ExpGlobal
    model_file: '{EXP_DIR}/models/{EXP}.mod'
    log_file: '{EXP_DIR}/logs/{EXP}.log'
    default_layer_dim: 512
    dropout: 0.3
    placeholders:
      DATA_IN: examples/data
      DATA_OUT: examples/preproc
  preproc: !PreprocRunner
    overwrite: False
    tasks:
    - !PreprocVocab
      in_files:
      - '{DATA_IN}/train.ja'
      - '{DATA_IN}/train.en'
      out_files:
      - '{DATA_OUT}/train.ja.vocab'
      - '{DATA_OUT}/train.en.vocab'
      specs:
      - filenum: all
        filters:
        - !VocabFiltererFreq
            min_freq: 2
  model: !DefaultTranslator
    src_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: '{DATA_OUT}/train.ja.vocab'}
    trg_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: '{DATA_OUT}/train.en.vocab'}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 512
    encoder: !ModularSeqTransducer
      modules:
      - !ResidualSeqTransducer
        input_dim: 512
        child: !MultiHeadAttentionSeqTransducer
          num_heads: 8
        layer_norm: True
      - !ResidualSeqTransducer
        input_dim: 512
        child: !ModularSeqTransducer
          input_dim: 512
          modules:
          - !TransformSeqTransducer
            transform: !NonLinear
              activation: relu
          - !TransformSeqTransducer
            transform: !Linear {}
        layer_norm: True
      - !ResidualSeqTransducer
        input_dim: 512
        child: !MultiHeadAttentionSeqTransducer
          num_heads: 8
        layer_norm: True
      - !ResidualSeqTransducer
        input_dim: 512
        child: !ModularSeqTransducer
          input_dim: 512
          modules:
          - !TransformSeqTransducer
            transform: !NonLinear
              activation: relu
          - !TransformSeqTransducer
            transform: !Linear {}
        layer_norm: True
    # encoder: !BiLSTMSeqTransducer
    #   layers: 1
    attender: !MlpAttender
      hidden_dim: 512
      state_dim: 512
      input_dim: 512
    trg_embedder: !SimpleWordEmbedder
      emb_dim: 512
    decoder: !AutoRegressiveDecoder
      rnn: !UniLSTMSeqTransducer
        layers: 1
      transform: !AuxNonLinear
        output_dim: 512
        activation: 'tanh'
      bridge: !CopyBridge {}
  train: !SimpleTrainingRegimen
    batcher: !SrcBatcher
      batch_size: 32
    trainer: !AdamTrainer
      alpha: 0.001
    run_for_epochs: 2
    src_file: examples/data/train.ja
    trg_file: examples/data/train.en
    dev_tasks:
      - !LossEvalTask
        src_file: examples/data/head.ja
        ref_file: examples/data/head.en
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: examples/data/head.ja
      ref_file: examples/data/head.en
      hyp_file: examples/output/{EXP}.test_hyp
