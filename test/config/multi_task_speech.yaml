# note: run as ``xnmt --settings=unittest <config>.yaml``
exp1-multi_task_exp: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 64
  train: !SameBatchMultiTaskTrainingRegimen
    trainer: !AdamTrainer {}
    tasks:
    - !SimpleTrainingTask # first task is the main task: it will control early stopping, learning rate schedule, model checkpoints, ..
      name: first_task
      run_for_epochs: 6
      batcher: !SrcBatcher
        src_pad_token: ~
        pad_src_to_multiple: 4
        batch_size: 6 # batch size is twice as big as for task 2, which will give task 1 more impact during training
      src_file: examples/data/LDC94S13A.h5
      trg_file: examples/data/LDC94S13A.char
      model: !DefaultTranslator
        _xnmt_id: first_task_model
        src_reader: !H5Reader
          transpose: True
        trg_reader: !PlainTextReader
          vocab: !Vocab
            _xnmt_id: trg_vocab
            vocab_file: examples/data/head.en.vocab
        src_embedder: !NoopEmbedder
          emb_dim: 40
        encoder: !PyramidalLSTMSeqTransducer # the encoder shares parameters between tasks
          _xnmt_id: first_task_encoder
          layers: 2
          downsampling_method: concat
          reduce_factor: 2
          input_dim: 40
          hidden_dim: 64
        attender: !MlpAttender
          state_dim: 64
          hidden_dim: 64
          input_dim: 64
        trg_embedder: !SimpleWordEmbedder
          emb_dim: 64
          vocab: !Ref {name: trg_vocab}
        decoder: !AutoRegressiveDecoder
          rnn: !UniLSTMSeqTransducer
            layers: 1
            hidden_dim: 64
          bridge: !CopyBridge {}
          scorer: !Softmax
            vocab: !Ref {name: trg_vocab}
        inference: !AutoRegressiveInference
          batcher: !InOrderBatcher
            _xnmt_id: inference_batcher
            src_pad_token: ~
            pad_src_to_multiple: 4
            batch_size: 1
      dev_tasks:
        - !AccuracyEvalTask
          model: !Ref { name: first_task_model }
          src_file: &first_task_dev_src examples/data/LDC94S13A.h5    # value-sharing between train.training_corpus.dev_src and inference.src_file
          ref_file: &first_task_dev_trg examples/data/LDC94S13A.char  # value-sharing between train.training_corpus.dev_trg and evaluate.ref_file
          hyp_file: test/tmp/{EXP}.first_dev_hyp
          eval_metrics: bleu # tasks can specify different dev_metrics
    - !SimpleTrainingTask
      name: second_task
      batcher: !SrcBatcher
        src_pad_token: ~
        batch_size: 3
        pad_src_to_multiple: 4
      src_file: examples/data/LDC94S13A.h5
      trg_file: examples/data/LDC94S13A.char
      model: !DefaultTranslator
        _xnmt_id: second_task_model
        src_reader: !H5Reader
          transpose: True
        trg_reader: !PlainTextReader
          vocab: !Ref {name: trg_vocab}
        src_embedder: !NoopEmbedder
          emb_dim: 40
        encoder: !Ref { name: first_task_encoder }
        attender: !MlpAttender
          state_dim: 64
          hidden_dim: 64
          input_dim: 64
        trg_embedder: !SimpleWordEmbedder
          emb_dim: 64
          vocab: !Ref {name: trg_vocab}
        decoder: !AutoRegressiveDecoder
          rnn: !UniLSTMSeqTransducer
            layers: 1
          bridge: !CopyBridge {}
          scorer: !Softmax
            vocab: !Ref {name: trg_vocab}
        inference: !AutoRegressiveInference
          batcher: !Ref { name: inference_batcher }
      dev_tasks:
        - !AccuracyEvalTask
          model: !Ref { name: second_task_model }
          src_file: examples/data/LDC94S13A.h5
          ref_file: examples/data/LDC94S13A.char  # value-sharing between train.training_corpus.dev_trg and evaluate.ref_file
          hyp_file: test/tmp/{EXP}.second_dev_hyp
          eval_metrics: gleu # tasks can specify different dev_metrics
  evaluate:
    - !AccuracyEvalTask
      model: !Ref { name: first_task_model }
      eval_metrics: bleu,wer
      src_file: *first_task_dev_src
      ref_file: *first_task_dev_trg
      hyp_file: test/tmp/{EXP}.test_hyp
      inference: !AutoRegressiveInference
        batcher: !Ref { name: inference_batcher }

exp2-finetune-model: !LoadSerialized
  filename: test/tmp/exp1-multi_task_exp.mod
