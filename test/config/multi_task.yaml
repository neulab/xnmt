# note: run as ``xnmt --settings=unittest <config>.yaml``

# Simple example for multi-task training. Here, we have 2 identical tasks with some shared parameters.
exp1-multi_task_exp: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 64
  train: !SameBatchMultiTaskTrainingRegimen
    trainer: !AdamTrainer {}
    tasks: &training_tasks
    - !SimpleTrainingTask # first task is the main task: it will control early stopping, learning rate schedule, model checkpoints, ..
      name: first_task
      run_for_epochs: 6
      batcher: !SrcBatcher
        batch_size: 6 # batch size is twice as big as for task 2, which will give task 1 more impact during training
      src_file: examples/data/head.ja
      trg_file: examples/data/head.en
      model: !DefaultTranslator
        _xnmt_id: first_task_model
        src_reader: !PlainTextReader
          vocab: !Vocab
            _xnmt_id: src_vocab
            vocab_file: examples/data/head.ja.vocab
        trg_reader: !PlainTextReader
          vocab: !Vocab
            _xnmt_id: trg_vocab
            vocab_file: examples/data/head.en.vocab
        src_embedder: !SimpleWordEmbedder
          emb_dim: 64
          vocab: !Ref {name: src_vocab}
        encoder: !BiLSTMSeqTransducer # the encoder shares parameters between tasks
          _xnmt_id: first_task_encoder
          layers: 1
        attender: !MlpAttender
          state_dim: 64
          hidden_dim: 64
          input_dim: 64
        trg_embedder: !SimpleWordEmbedder
          emb_dim: 64
          vocab: !Ref {name: trg_vocab}
        decoder: !AutoRegressiveDecoder
          rnn: !UniLSTMSeqTransducer
            layers: 1
            hidden_dim: 64
          bridge: !CopyBridge {}
          scorer: !Softmax
            vocab: !Ref {name: trg_vocab}
      dev_tasks:
        - !AccuracyEvalTask
          model: !Ref { name: first_task_model }
          src_file: &first_task_dev_src examples/data/head.ja  # value-sharing between first task dev and final eval
          ref_file: &first_task_dev_trg examples/data/head.en  # value-sharing between first task dev and final eval
          hyp_file: test/tmp/{EXP}.first_dev_hyp
          eval_metrics: bleu # tasks can specify different dev_metrics
    - !SimpleTrainingTask
      name: second_task
      batcher: !SrcBatcher
        batch_size: 3
      src_file: examples/data/head.ja
      trg_file: examples/data/head.en
      run_for_epochs: 6
      model: !DefaultTranslator
        _xnmt_id: second_task_model
        src_reader: !PlainTextReader
          vocab: !Ref {name: src_vocab}
        trg_reader: !PlainTextReader
          vocab: !Ref {name: trg_vocab}
        src_embedder: !SimpleWordEmbedder
          emb_dim: 64
          vocab: !Ref {name: src_vocab}
        encoder: !Ref { name: first_task_encoder }
        attender: !MlpAttender
          state_dim: 64
          hidden_dim: 64
          input_dim: 64
        trg_embedder: !SimpleWordEmbedder
          emb_dim: 64
          vocab: !Ref {name: trg_vocab}
        decoder: !AutoRegressiveDecoder
          rnn: !UniLSTMSeqTransducer
            layers: 1
          bridge: !CopyBridge {}
          scorer: !Softmax
            vocab: !Ref {name: trg_vocab}
      dev_tasks:
        - !AccuracyEvalTask
          model: !Ref { name: second_task_model }
          src_file: examples/data/head.ja
          ref_file: examples/data/head.en
          hyp_file: test/tmp/{EXP}.second_dev_hyp
          eval_metrics: gleu # tasks can specify different dev_metrics
  evaluate:
    - !AccuracyEvalTask
      model: !Ref { name: first_task_model }
      eval_metrics: bleu,wer
      src_file: *first_task_dev_src
      ref_file: *first_task_dev_trg
      hyp_file: test/tmp/{EXP}.test_hyp

exp2-finetune-model: !LoadSerialized
  filename: test/tmp/exp1-multi_task_exp.mod

exp3-multi_task_exp-altbatch: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 64
  train: !AlternatingBatchMultiTaskTrainingRegimen
    trainer: !AdamTrainer {}
    tasks: *training_tasks

exp4-multi_task_exp-serial: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 64
  train: !SerialMultiTaskTrainingRegimen
    trainer: !AdamTrainer {}
    tasks: *training_tasks
