# note: run as ``xnmt --settings=unittest <config>.yaml``
exp1-pretrain-model: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 64
    dropout: 0.5
    weight_noise: 0.1
  model: !DefaultTranslator
    src_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: examples/data/head.ja.vocab}
    trg_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: examples/data/head.en.vocab}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 64
    encoder: !BiLSTMSeqTransducer
      layers: 2
      input_dim: 64
    attender: !MlpAttender
      state_dim: 64
      hidden_dim: 64
      input_dim: 64
    trg_embedder: !SimpleWordEmbedder
      emb_dim: 64
    decoder: !AutoRegressiveDecoder
      rnn: !UniLSTMSeqTransducer
        layers: 1
      bridge: !CopyBridge {}
    inference: !AutoRegressiveInference {}
  train: !SimpleTrainingRegimen
    run_for_epochs: 2
    src_file: examples/data/head.ja
    trg_file: examples/data/head.en
    dev_tasks:
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: examples/data/head.ja
      ref_file: examples/data/head.en
      hyp_file: test/tmp/{EXP}.dev_hyp
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu
      src_file: examples/data/head.ja
      ref_file: examples/data/head.en
      hyp_file: test/tmp/{EXP}.test_hyp

exp2-finetune-model: !LoadSerialized
  # This will load the contents of the above experiments that were saved to the
  # YAML file specified after filename:
  # This will carry out the exact same thing, except that {EXP} is resolved to
  # a different value (making sure we don't overwrite the previous model),
  # and except for the things explicitly overwritten in the overwrite: section.
  # It's possible to change any settings as long as these don't change the number
  # or nature of DyNet parameters allocated for the component.
  filename: test/tmp/exp1-pretrain-model.mod
  path: ''
  overwrite: # list of [path, value] pairs. Value can be scalar or an arbitrary object
  - path: train.trainer
    val: !AdamTrainer
        alpha: 0.0002
  - path: exp_global.dropout
    val: 0.5

