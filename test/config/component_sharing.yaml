exp1.pretrain: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 32
    model_file: "test/tmp/{EXP}.mod"
  model: !DefaultTranslator
    src_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: examples/data/head.ja.vocab}
    trg_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: examples/data/head.en.vocab}
    src_embedder: !SimpleWordEmbedder
      emb_dim: 32
    encoder: !ModularSeqTransducer
      modules:
      - !BiLSTMSeqTransducer
        _xnmt_id: shared_enc
        layers: 1
      - !Ref { name: shared_enc }
    attender: !MlpAttender
      input_dim: 32
    trg_embedder: !DenseWordEmbedder
      _xnmt_id: trg_emb
      emb_dim: 32
    decoder: !AutoRegressiveDecoder
      rnn: !UniLSTMSeqTransducer
        layers: 1
      bridge: !CopyBridge {}
      scorer: !Softmax
        output_projector: !Ref { name: trg_emb }
    inference: !AutoRegressiveInference {}
  train: !SimpleTrainingRegimen
    run_for_epochs: 2
    src_file: examples/data/head.ja
    trg_file: examples/data/head.en
    dev_tasks:
      - !LossEvalTask
        src_file: &dev_src examples/data/head.ja
        ref_file: &dev_trg examples/data/head.en
  evaluate:
    - !AccuracyEvalTask
      eval_metrics: bleu,wer
      src_file: *dev_src
      ref_file: *dev_trg
      hyp_file: test/tmp/{EXP}.test_hyp

exp2.load: !LoadSerialized
  filename: test/tmp/exp1.pretrain.mod
  
