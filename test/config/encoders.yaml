# Small config to help refactoring to make the model completely YAML based
defaults:
  experiment:
    model_file: examples/output/<EXP>.mod
    hyp_file: examples/output/<EXP>.hyp
    out_file: examples/output/<EXP>.out
    err_file: examples/output/<EXP>.err
    run_for_epochs: 2
    eval_metrics: bleu
  train:
    default_layer_dim: 64
    dropout: 0.5
    weight_noise: 0.1
    dev_metrics: bleu
    training_corpus: !BilingualTrainingCorpus
      train_src: examples/data/head.ja
      train_trg: examples/data/head.en
      dev_src: examples/data/head.ja
      dev_trg: examples/data/head.en
    corpus_parser: !BilingualCorpusParser
      src_reader: !PlainTextReader {}
      trg_reader: !PlainTextReader {}
      max_src_len: 15
      max_trg_len: 15
  decode:
    src_file: examples/data/head.ja
  evaluate:
    ref_file: examples/data/head.en

exp1-lstm-encoder:
  train:
    model: !DefaultTranslator
      src_embedder: !SimpleWordEmbedder
        emb_dim: 64
        word_dropout: 0.3
      encoder: !LSTMSeqTransducer
        layers: 2
        input_dim: 64
        bidirectional: True
      attender: !StandardAttender
        state_dim: 64
        hidden_dim: 64
        input_dim: 64
      trg_embedder: !SimpleWordEmbedder
        emb_dim: 64
        word_dropout: 0.3
      decoder: !MlpSoftmaxDecoder
        layers: 1
        mlp_hidden_dim: 64
        input_feeding: True
        bridge: !CopyBridge {}
exp2-residual-encoder:
  train:
    model: !DefaultTranslator
      src_embedder: !SimpleWordEmbedder
        emb_dim: 64
      encoder: !ResidualLSTMSeqTransducer
        layers: 2
        input_dim: 64
        bidirectional: True
      attender: !StandardAttender
        state_dim: 64
        hidden_dim: 64
        input_dim: 64
      trg_embedder: !SimpleWordEmbedder
        emb_dim: 64
      decoder: !MlpSoftmaxDecoder
        layers: 2
        mlp_hidden_dim: 64
        input_feeding: True
        bridge: !CopyBridge {}
exp3-pyramidal-encoder:
  train:
    model: !DefaultTranslator
      src_embedder: !SimpleWordEmbedder
        emb_dim: 64
      encoder: !PyramidalLSTMSeqTransducer
        layers: 3
        input_dim: 64
      attender: !StandardAttender
        state_dim: 64
        hidden_dim: 64
        input_dim: 64
      trg_embedder: !SimpleWordEmbedder
        emb_dim: 64
      decoder: !MlpSoftmaxDecoder
        layers: 2
        mlp_hidden_dim: 64
        input_feeding: True
        bridge: !CopyBridge {}

exp4-modular-encoder:
  train:
    model: !DefaultTranslator
      src_embedder: !SimpleWordEmbedder
        emb_dim: 64
      encoder: !ModularSeqTransducer
        modules:
        - !LSTMSeqTransducer
            input_dim: 64
            hidden_dim: 64
            layers: 1
            dropout: 0.1
        - !PyramidalLSTMSeqTransducer
            input_dim: 64
            hidden_dim: 64
            layers: 2
      attender: !StandardAttender
        state_dim: 64
        hidden_dim: 64
        input_dim: 64
      trg_embedder: !SimpleWordEmbedder
        emb_dim: 64
      decoder: !MlpSoftmaxDecoder
        layers: 3
        mlp_hidden_dim: 64
        input_feeding: True
        bridge: !CopyBridge {}

