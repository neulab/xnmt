# An example for how to use pretrained embeddings
pretrained_emb: !Experiment
  exp_global: !ExpGlobal
    default_layer_dim: 64
  model: !DefaultTranslator
    src_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: test/data/head.ja.vocab}
    trg_reader: !PlainTextReader
      vocab: !Vocab {vocab_file: test/data/head.en.vocab}
    src_embedder: !PretrainedSimpleWordEmbedder
      filename: test/data/wiki.ja.vec.small
      emb_dim: 300
      vocab: !Ref {path: model.src_reader.vocab}
    encoder: !BiLSTMSeqTransducer
      layers: 1
    attender: !MlpAttender
      hidden_dim: 512
      state_dim: 512
      input_dim: 512
    decoder: !AutoRegressiveDecoder
      embedder: !SimpleWordEmbedder
        emb_dim: 512
  train: !SimpleTrainingRegimen
    run_for_epochs: 2
    restart_trainer: True
    trainer: !AdamTrainer
      alpha: 0.0002
    lr_decay: 0.5
    src_file: test/data/head.ja
    trg_file: test/data/head.en
    dev_tasks:
      - !LossEvalTask
        src_file: test/data/head.ja
        ref_file: test/data/head.en
